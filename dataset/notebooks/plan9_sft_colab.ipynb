{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plan 9 Programming - Multi-Stage Fine-tuning\n",
    "\n",
    "Three-stage training pipeline for teaching LLMs Plan 9 programming:\n",
    "\n",
    "| Stage | Dataset | Purpose |\n",
    "|-------|---------|--------|\n",
    "| 1. Knowledge | `knowledge.jsonl` | Inject Plan 9 source code patterns |\n",
    "| 2. SFT | `conversations.jsonl` | Learn multi-turn tool use |\n",
    "| 3. GRPO | Remote QEMU API | Optimize with execution rewards |\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/garutyunov/plan9-dataset/blob/main/notebooks/plan9_sft_colab.ipynb)\n",
    "\n",
    "## Requirements\n",
    "- Google Colab with T4 GPU (free tier)\n",
    "- For GRPO: Remote server running `plan9-dataset serve-qemu`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install unsloth\n",
    "!pip install --no-deps trl peft accelerate bitsandbytes\n",
    "!pip install datasets huggingface-hub requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset configuration\n",
    "DATASET_REPO = \"garutyunov/plan9-sft\"\n",
    "\n",
    "# Training stages to run\n",
    "RUN_KNOWLEDGE_STAGE = True   # Stage 1: Continued pretraining on source code\n",
    "RUN_SFT_STAGE = True         # Stage 2: SFT on multi-turn conversations\n",
    "RUN_GRPO_STAGE = False       # Stage 3: GRPO with execution rewards (requires server)\n",
    "\n",
    "# Model settings\n",
    "MODEL_NAME = \"unsloth/gemma-3-1b-it-bnb-4bit\"\n",
    "MAX_SEQ_LENGTH = 2048\n",
    "LOAD_IN_4BIT = True\n",
    "\n",
    "# LoRA settings\n",
    "LORA_R = 8\n",
    "LORA_ALPHA = 16\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Knowledge stage: {RUN_KNOWLEDGE_STAGE}\")\n",
    "print(f\"  SFT stage: {RUN_SFT_STAGE}\")\n",
    "print(f\"  GRPO stage: {RUN_GRPO_STAGE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=MODEL_NAME,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dtype=None,\n",
    "    load_in_4bit=LOAD_IN_4BIT,\n",
    ")\n",
    "\n",
    "# Add LoRA adapters\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=LORA_R,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load all datasets\n",
    "print(\"Loading datasets...\")\n",
    "\n",
    "# Knowledge dataset (raw source code for continued pretraining)\n",
    "try:\n",
    "    knowledge_ds = load_dataset(DATASET_REPO, data_files=\"knowledge.jsonl\")[\"train\"]\n",
    "    print(f\"✓ Knowledge: {len(knowledge_ds)} files\")\n",
    "except:\n",
    "    knowledge_ds = None\n",
    "    print(\"✗ Knowledge dataset not found\")\n",
    "\n",
    "# Conversations dataset (multi-turn SFT)\n",
    "try:\n",
    "    conversations_ds = load_dataset(DATASET_REPO, data_files=\"conversations.jsonl\")[\"train\"]\n",
    "    print(f\"✓ Conversations: {len(conversations_ds)} examples\")\n",
    "except:\n",
    "    conversations_ds = None\n",
    "    print(\"✗ Conversations dataset not found\")\n",
    "\n",
    "# Simple SFT dataset (fallback)\n",
    "try:\n",
    "    simple_ds = load_dataset(DATASET_REPO, data_files=\"dataset.jsonl\")[\"train\"]\n",
    "    print(f\"✓ Simple SFT: {len(simple_ds)} examples\")\n",
    "except:\n",
    "    simple_ds = None\n",
    "    print(\"✗ Simple SFT dataset not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Stage 1: Knowledge Injection (Continued Pretraining)\n",
    "\n",
    "Train on raw Plan 9 source code to inject programming patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_KNOWLEDGE_STAGE and knowledge_ds is not None:\n",
    "    from trl import SFTTrainer\n",
    "    from transformers import TrainingArguments\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Stage 1: Knowledge Injection\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Format: just raw text (no chat template)\n",
    "    def format_knowledge(example):\n",
    "        # Add file type header\n",
    "        header = f\"/* Plan 9 {example['file_type']} - {example['source']} */\\n\"\n",
    "        return {\"text\": header + example[\"text\"]}\n",
    "\n",
    "    knowledge_formatted = knowledge_ds.map(format_knowledge)\n",
    "\n",
    "    # Training args for continued pretraining\n",
    "    knowledge_args = TrainingArguments(\n",
    "        output_dir=\"./plan9-knowledge\",\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=8,\n",
    "        warmup_steps=50,\n",
    "        num_train_epochs=1,\n",
    "        learning_rate=5e-5,  # Lower LR for pretraining\n",
    "        fp16=not torch.cuda.is_bf16_supported(),\n",
    "        bf16=torch.cuda.is_bf16_supported(),\n",
    "        logging_steps=25,\n",
    "        save_strategy=\"epoch\",\n",
    "        optim=\"adamw_8bit\",\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "\n",
    "    knowledge_trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=knowledge_formatted,\n",
    "        dataset_text_field=\"text\",\n",
    "        max_seq_length=MAX_SEQ_LENGTH,\n",
    "        packing=True,  # Pack short examples for efficiency\n",
    "        args=knowledge_args,\n",
    "    )\n",
    "\n",
    "    print(f\"Training on {len(knowledge_formatted)} source files...\")\n",
    "    knowledge_trainer.train()\n",
    "    print(\"✓ Stage 1 complete\")\n",
    "else:\n",
    "    print(\"Skipping Stage 1 (Knowledge)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Stage 2: Supervised Fine-Tuning (SFT)\n",
    "\n",
    "Train on multi-turn conversations with tool calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_SFT_STAGE:\n",
    "    from trl import SFTTrainer\n",
    "    from transformers import TrainingArguments\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Stage 2: Supervised Fine-Tuning\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Use conversations if available, else fall back to simple format\n",
    "    if conversations_ds is not None:\n",
    "        print(\"Using multi-turn conversations dataset\")\n",
    "\n",
    "        def format_conversation(example):\n",
    "            \"\"\"Format multi-turn conversation for training.\"\"\"\n",
    "            turns = example[\"turns\"]\n",
    "            text_parts = []\n",
    "\n",
    "            for turn in turns:\n",
    "                role = turn[\"role\"]\n",
    "                content = turn[\"content\"]\n",
    "\n",
    "                if role == \"user\":\n",
    "                    text_parts.append(f\"<start_of_turn>user\\n{content}<end_of_turn>\")\n",
    "                elif role == \"model\":\n",
    "                    # Include thinking and tool calls\n",
    "                    model_text = \"\"\n",
    "                    if turn.get(\"thinking\"):\n",
    "                        model_text += f\"<think>\\n{turn['thinking']}\\n</think>\\n\"\n",
    "                    if turn.get(\"tool_calls\"):\n",
    "                        for tc in turn[\"tool_calls\"]:\n",
    "                            import json\n",
    "                            params = json.dumps(tc[\"params\"])\n",
    "                            model_text += f\"<start_function_call>call:{tc['name']}{params}<end_function_call>\\n\"\n",
    "                    if content:\n",
    "                        model_text += content\n",
    "                    text_parts.append(f\"<start_of_turn>model\\n{model_text.strip()}<end_of_turn>\")\n",
    "                elif role == \"tool\":\n",
    "                    text_parts.append(f\"<start_of_turn>user\\n{content}<end_of_turn>\")\n",
    "\n",
    "            return {\"text\": \"\\n\".join(text_parts)}\n",
    "\n",
    "        sft_dataset = conversations_ds.map(format_conversation)\n",
    "\n",
    "    elif simple_ds is not None:\n",
    "        print(\"Using simple instruction-response dataset\")\n",
    "\n",
    "        def format_simple(example):\n",
    "            return {\n",
    "                \"text\": f\"<start_of_turn>user\\n{example['instruction']}<end_of_turn>\\n<start_of_turn>model\\n{example['response']}<end_of_turn>\"\n",
    "            }\n",
    "\n",
    "        sft_dataset = simple_ds.map(format_simple)\n",
    "    else:\n",
    "        raise ValueError(\"No SFT dataset available!\")\n",
    "\n",
    "    # Training args for SFT\n",
    "    sft_args = TrainingArguments(\n",
    "        output_dir=\"./plan9-sft\",\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=8,\n",
    "        warmup_steps=10,\n",
    "        num_train_epochs=3,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=not torch.cuda.is_bf16_supported(),\n",
    "        bf16=torch.cuda.is_bf16_supported(),\n",
    "        logging_steps=10,\n",
    "        save_strategy=\"epoch\",\n",
    "        optim=\"adamw_8bit\",\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "\n",
    "    sft_trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=sft_dataset,\n",
    "        dataset_text_field=\"text\",\n",
    "        max_seq_length=MAX_SEQ_LENGTH,\n",
    "        packing=False,\n",
    "        args=sft_args,\n",
    "    )\n",
    "\n",
    "    print(f\"Training on {len(sft_dataset)} examples...\")\n",
    "    sft_trainer.train()\n",
    "    print(\"✓ Stage 2 complete\")\n",
    "else:\n",
    "    print(\"Skipping Stage 2 (SFT)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Stage 3: GRPO with Remote Execution Rewards\n",
    "\n",
    "Optimize using real Plan 9 execution feedback from remote QEMU server.\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "1. **On your server with QEMU:**\n",
    "   ```bash\n",
    "   pip install 'plan9-dataset[server]'\n",
    "   plan9-dataset serve-qemu --generate-token\n",
    "   plan9-dataset serve-qemu --token YOUR_TOKEN --port 8080\n",
    "   ```\n",
    "\n",
    "2. **In Colab Secrets** (key icon in sidebar):\n",
    "   - `QEMU_SERVER_URL`: e.g., `https://your-server.com:8080`\n",
    "   - `QEMU_TOKEN`: Token from step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QEMU API Client\n",
    "import requests\n",
    "import json\n",
    "\n",
    "class QEMUClient:\n",
    "    \"\"\"Client for remote Plan 9 QEMU API.\"\"\"\n",
    "\n",
    "    def __init__(self, server_url: str, token: str):\n",
    "        self.server_url = server_url.rstrip(\"/\")\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            \"Authorization\": f\"Bearer {token}\",\n",
    "            \"Content-Type\": \"application/json\",\n",
    "        })\n",
    "\n",
    "    def health(self) -> dict:\n",
    "        \"\"\"Check server health.\"\"\"\n",
    "        r = self.session.get(f\"{self.server_url}/health\", timeout=10)\n",
    "        r.raise_for_status()\n",
    "        return r.json()\n",
    "\n",
    "    def execute(self, tool: str, params: dict) -> dict:\n",
    "        \"\"\"Execute a tool on the remote VM.\"\"\"\n",
    "        r = self.session.post(\n",
    "            f\"{self.server_url}/execute\",\n",
    "            json={\"tool\": tool, \"params\": params},\n",
    "            timeout=60,\n",
    "        )\n",
    "        r.raise_for_status()\n",
    "        return r.json()\n",
    "\n",
    "    def write_file(self, path: str, content: str) -> dict:\n",
    "        \"\"\"Write a file on the remote VM.\"\"\"\n",
    "        return self.execute(\"write_file\", {\"path\": path, \"content\": content})\n",
    "\n",
    "    def read_file(self, path: str) -> dict:\n",
    "        \"\"\"Read a file from the remote VM.\"\"\"\n",
    "        return self.execute(\"read_file\", {\"path\": path})\n",
    "\n",
    "    def run_command(self, command: str) -> dict:\n",
    "        \"\"\"Run a command on the remote VM.\"\"\"\n",
    "        return self.execute(\"run_command\", {\"command\": command})\n",
    "\n",
    "    def reset(self) -> dict:\n",
    "        \"\"\"Reset VM state.\"\"\"\n",
    "        r = self.session.post(f\"{self.server_url}/reset\", timeout=30)\n",
    "        r.raise_for_status()\n",
    "        return r.json()\n",
    "\n",
    "    def compute_reward(self, model_output: str, expected_output: str = None) -> dict:\n",
    "        \"\"\"Compute reward for model output using VM execution.\"\"\"\n",
    "        r = self.session.post(\n",
    "            f\"{self.server_url}/reward\",\n",
    "            json={\"model_output\": model_output, \"expected_output\": expected_output},\n",
    "            timeout=120,\n",
    "        )\n",
    "        r.raise_for_status()\n",
    "        return r.json()\n",
    "\n",
    "print(\"QEMUClient defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GRPO configuration\n",
    "GRPO_READY = False\n",
    "\n",
    "if RUN_GRPO_STAGE:\n",
    "    try:\n",
    "        from google.colab import userdata\n",
    "        QEMU_SERVER_URL = userdata.get('QEMU_SERVER_URL')\n",
    "        QEMU_TOKEN = userdata.get('QEMU_TOKEN')\n",
    "\n",
    "        if QEMU_SERVER_URL and QEMU_TOKEN:\n",
    "            client = QEMUClient(QEMU_SERVER_URL, QEMU_TOKEN)\n",
    "            health = client.health()\n",
    "            print(f\"✓ Connected to QEMU server\")\n",
    "            print(f\"  URL: {QEMU_SERVER_URL}\")\n",
    "            print(f\"  VM running: {health.get('vm_running')}\")\n",
    "            GRPO_READY = True\n",
    "        else:\n",
    "            print(\"✗ QEMU secrets not configured\")\n",
    "            print(\"  Add QEMU_SERVER_URL and QEMU_TOKEN to Colab secrets\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ GRPO setup failed: {e}\")\n",
    "else:\n",
    "    print(\"GRPO stage disabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GRPO_READY:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Stage 3: GRPO with Execution Rewards\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Load GRPO prompts\n",
    "    grpo_prompts = [\n",
    "        \"Write a Plan 9 C program that prints 'Hello, Plan 9!'\",\n",
    "        \"Write a Plan 9 C program that reads /dev/user and prints the username\",\n",
    "        \"Write an rc script that counts .c files in the current directory\",\n",
    "        \"Write a Plan 9 C program that calculates factorial of 5\",\n",
    "        \"Write a Plan 9 C program using channels to send integers between threads\",\n",
    "    ]\n",
    "\n",
    "    def reward_function(samples, prompts, outputs, **kwargs):\n",
    "        \"\"\"Compute rewards using remote QEMU execution.\"\"\"\n",
    "        rewards = []\n",
    "        for i, output in enumerate(outputs):\n",
    "            try:\n",
    "                result = client.compute_reward(output)\n",
    "                reward = result.get(\"total\", 0.0)\n",
    "                print(f\"  Sample {i}: reward={reward:.2f}\")\n",
    "                rewards.append(reward)\n",
    "                client.reset()\n",
    "            except Exception as e:\n",
    "                print(f\"  Sample {i}: error - {e}\")\n",
    "                rewards.append(0.0)\n",
    "        return rewards\n",
    "\n",
    "    # Create dataset from prompts\n",
    "    from datasets import Dataset\n",
    "\n",
    "    grpo_dataset = Dataset.from_dict({\n",
    "        \"prompt\": [f\"<start_of_turn>user\\n{p}<end_of_turn>\\n<start_of_turn>model\\n\" for p in grpo_prompts]\n",
    "    })\n",
    "\n",
    "    print(f\"GRPO dataset: {len(grpo_dataset)} prompts\")\n",
    "\n",
    "    # GRPO training\n",
    "    from trl import GRPOConfig, GRPOTrainer\n",
    "\n",
    "    grpo_config = GRPOConfig(\n",
    "        output_dir=\"./plan9-grpo\",\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=2,\n",
    "        num_train_epochs=2,\n",
    "        learning_rate=1e-5,\n",
    "        logging_steps=1,\n",
    "        num_generations=2,\n",
    "        temperature=0.8,\n",
    "        max_new_tokens=512,\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "\n",
    "    grpo_trainer = GRPOTrainer(\n",
    "        model=model,\n",
    "        config=grpo_config,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=grpo_dataset,\n",
    "        reward_funcs=[reward_function],\n",
    "    )\n",
    "\n",
    "    print(\"Starting GRPO training...\")\n",
    "    grpo_trainer.train()\n",
    "    print(\"✓ Stage 3 complete\")\n",
    "else:\n",
    "    print(\"Skipping Stage 3 (GRPO)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable inference mode\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "test_prompts = [\n",
    "    \"Write a Plan 9 C program that prints 'Hello, Plan 9!'\",\n",
    "    \"Write an rc script that lists all .c files\",\n",
    "    \"How do I read a file using Bio in Plan 9 C?\",\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "    )\n",
    "\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    if \"model\" in response:\n",
    "        response = response.split(\"model\")[-1].strip()\n",
    "    print(response[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save locally\n",
    "model.save_pretrained(\"plan9-gemma-lora\")\n",
    "tokenizer.save_pretrained(\"plan9-gemma-lora\")\n",
    "print(\"✓ Saved to plan9-gemma-lora/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Push to HuggingFace Hub\n",
    "PUSH_TO_HUB = False\n",
    "HUB_REPO = \"YOUR_USERNAME/plan9-gemma-lora\"\n",
    "\n",
    "if PUSH_TO_HUB:\n",
    "    from huggingface_hub import login\n",
    "    login()\n",
    "    model.push_to_hub(HUB_REPO)\n",
    "    tokenizer.push_to_hub(HUB_REPO)\n",
    "    print(f\"✓ Pushed to {HUB_REPO}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Resources\n",
    "\n",
    "- [Plan 9 Dataset](https://huggingface.co/datasets/garutyunov/plan9-sft)\n",
    "- [9ml Project](https://github.com/garutyunov/9ml)\n",
    "- [Unsloth](https://github.com/unslothai/unsloth)\n",
    "- [TRL Documentation](https://huggingface.co/docs/trl)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
