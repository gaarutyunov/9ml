{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Plan 9 Programming - Fine-tuning with Unsloth\n",
        "\n",
        "This notebook fine-tunes Gemma 3 1B on the Plan 9 programming dataset using:\n",
        "- **Unsloth** for efficient LoRA training\n",
        "- **TRL** for SFT training\n",
        "- Optional **GRPO** with remote execution rewards\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/YOUR_USERNAME/plan9-dataset/blob/main/notebooks/plan9_sft_colab.ipynb)\n",
        "\n",
        "## Requirements\n",
        "- Google Colab with T4 GPU (free tier works)\n",
        "- HuggingFace account for model access"
      ],
      "metadata": {
        "id": "header"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Install Dependencies"
      ],
      "metadata": {
        "id": "install-header"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install-deps"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install unsloth\n",
        "!pip install --no-deps trl peft accelerate bitsandbytes\n",
        "!pip install datasets huggingface-hub requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify GPU\n",
        "import torch\n",
        "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
      ],
      "metadata": {
        "id": "verify-gpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Load Model with 4-bit Quantization"
      ],
      "metadata": {
        "id": "model-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "\n",
        "# Model settings optimized for T4 GPU\n",
        "max_seq_length = 2048\n",
        "dtype = None  # Auto-detect\n",
        "load_in_4bit = True\n",
        "\n",
        "# Load Gemma 3 1B\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/gemma-3-1b-it-bnb-4bit\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=load_in_4bit,\n",
        ")\n",
        "\n",
        "print(f\"Model loaded: {model.config._name_or_path}\")"
      ],
      "metadata": {
        "id": "load-model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Add LoRA Adapters"
      ],
      "metadata": {
        "id": "lora-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add LoRA adapters - conservative settings for T4\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=8,  # LoRA rank (lower = less memory)\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "    ],\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",  # Memory optimization\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "# Print trainable parameters\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "add-lora"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Load Dataset from HuggingFace Hub"
      ],
      "metadata": {
        "id": "dataset-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load Plan 9 dataset\n",
        "# Change this to your dataset repo\n",
        "DATASET_REPO = \"garutyunov/plan9-sft\"\n",
        "\n",
        "# Load SFT format (simple instruction-response pairs)\n",
        "dataset = load_dataset(DATASET_REPO, \"sft\")\n",
        "\n",
        "print(f\"Loaded {len(dataset['train'])} training examples\")\n",
        "print(f\"\\nSample example:\")\n",
        "print(f\"Instruction: {dataset['train'][0]['instruction'][:100]}...\")\n",
        "print(f\"Response: {dataset['train'][0]['response'][:100]}...\")"
      ],
      "metadata": {
        "id": "load-dataset"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Format examples for training\n",
        "def format_prompt(example):\n",
        "    \"\"\"Format example as Gemma chat template.\"\"\"\n",
        "    instruction = example[\"instruction\"]\n",
        "    response = example[\"response\"]\n",
        "\n",
        "    # Gemma chat format\n",
        "    text = f\"\"\"<start_of_turn>user\n",
        "{instruction}<end_of_turn>\n",
        "<start_of_turn>model\n",
        "{response}<end_of_turn>\"\"\"\n",
        "\n",
        "    return {\"text\": text}\n",
        "\n",
        "# Apply formatting\n",
        "formatted_dataset = dataset[\"train\"].map(format_prompt)\n",
        "print(f\"\\nFormatted example:\")\n",
        "print(formatted_dataset[0][\"text\"][:500])"
      ],
      "metadata": {
        "id": "format-dataset"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Configure SFT Trainer"
      ],
      "metadata": {
        "id": "trainer-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "# Training arguments optimized for T4 (16GB VRAM)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./plan9-gemma-lora\",\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=8,  # Effective batch size = 8\n",
        "    warmup_steps=10,\n",
        "    num_train_epochs=3,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=not torch.cuda.is_bf16_supported(),\n",
        "    bf16=torch.cuda.is_bf16_supported(),\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    optim=\"adamw_8bit\",\n",
        "    weight_decay=0.01,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    seed=42,\n",
        "    report_to=\"none\",  # Disable wandb in Colab\n",
        ")\n",
        "\n",
        "# Create trainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=formatted_dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dataset_num_proc=2,\n",
        "    packing=False,  # Can enable for efficiency if examples are short\n",
        "    args=training_args,\n",
        ")\n",
        "\n",
        "print(\"Trainer configured!\")"
      ],
      "metadata": {
        "id": "configure-trainer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Train"
      ],
      "metadata": {
        "id": "train-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "trainer_stats = trainer.train()\n",
        "\n",
        "print(f\"\\nTraining complete!\")\n",
        "print(f\"Total steps: {trainer_stats.global_step}\")\n",
        "print(f\"Training loss: {trainer_stats.training_loss:.4f}\")"
      ],
      "metadata": {
        "id": "train"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Test Inference"
      ],
      "metadata": {
        "id": "inference-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Enable inference mode\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# Test prompts\n",
        "test_prompts = [\n",
        "    \"Write a Plan 9 C program that prints 'Hello, Plan 9!'\",\n",
        "    \"Write an rc script that lists all .c files in the current directory\",\n",
        "    \"How do I read a file line by line in Plan 9 C using Bio?\",\n",
        "]\n",
        "\n",
        "for prompt in test_prompts:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Format as chat\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=True,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    # Generate\n",
        "    outputs = model.generate(\n",
        "        input_ids=inputs,\n",
        "        max_new_tokens=512,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        do_sample=True,\n",
        "    )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    # Extract just the model response\n",
        "    if \"model\" in response:\n",
        "        response = response.split(\"model\")[-1].strip()\n",
        "    print(response[:1000])"
      ],
      "metadata": {
        "id": "test-inference"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Save Model"
      ],
      "metadata": {
        "id": "save-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save LoRA adapters locally\n",
        "model.save_pretrained(\"plan9-gemma-lora\")\n",
        "tokenizer.save_pretrained(\"plan9-gemma-lora\")\n",
        "\n",
        "print(\"Model saved to plan9-gemma-lora/\")"
      ],
      "metadata": {
        "id": "save-local"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: Push to HuggingFace Hub\n",
        "# Uncomment and set your repo name\n",
        "\n",
        "# from huggingface_hub import login\n",
        "# login()  # Will prompt for token\n",
        "\n",
        "# model.push_to_hub(\"YOUR_USERNAME/plan9-gemma-lora\")\n",
        "# tokenizer.push_to_hub(\"YOUR_USERNAME/plan9-gemma-lora\")"
      ],
      "metadata": {
        "id": "push-hub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Optional: GRPO Training with Remote Execution\n",
        "\n",
        "The following cells enable GRPO (Group Relative Policy Optimization) training with execution-based rewards. This requires a remote server running the Plan 9 QEMU API.\n",
        "\n",
        "## Setup\n",
        "\n",
        "1. On your server with QEMU:\n",
        "   ```bash\n",
        "   pip install 'plan9-dataset[server]'\n",
        "   plan9-dataset serve-qemu --generate-token\n",
        "   plan9-dataset serve-qemu --token YOUR_TOKEN --port 8080\n",
        "   ```\n",
        "\n",
        "2. Set Colab secrets:\n",
        "   - `QEMU_SERVER_URL`: Your server URL (e.g., `https://your-server.com:8080`)\n",
        "   - `QEMU_TOKEN`: The token from step 1"
      ],
      "metadata": {
        "id": "grpo-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if GRPO is configured\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "try:\n",
        "    QEMU_SERVER_URL = userdata.get('QEMU_SERVER_URL')\n",
        "    QEMU_TOKEN = userdata.get('QEMU_TOKEN')\n",
        "    GRPO_ENABLED = bool(QEMU_SERVER_URL and QEMU_TOKEN)\n",
        "except:\n",
        "    GRPO_ENABLED = False\n",
        "\n",
        "if GRPO_ENABLED:\n",
        "    print(\"✓ GRPO secrets configured\")\n",
        "    print(f\"  Server: {QEMU_SERVER_URL}\")\n",
        "else:\n",
        "    print(\"✗ GRPO not configured (missing secrets)\")\n",
        "    print(\"  Set QEMU_SERVER_URL and QEMU_TOKEN in Colab secrets to enable\")"
      ],
      "metadata": {
        "id": "check-grpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Skip this cell if GRPO is not configured\n",
        "if not GRPO_ENABLED:\n",
        "    print(\"Skipping GRPO setup - secrets not configured\")\n",
        "else:\n",
        "    import requests\n",
        "\n",
        "    class RemoteQEMUClient:\n",
        "        \"\"\"Simple client for remote QEMU API.\"\"\"\n",
        "\n",
        "        def __init__(self, server_url, token):\n",
        "            self.server_url = server_url.rstrip(\"/\")\n",
        "            self.session = requests.Session()\n",
        "            self.session.headers.update({\n",
        "                \"Authorization\": f\"Bearer {token}\",\n",
        "                \"Content-Type\": \"application/json\",\n",
        "            })\n",
        "\n",
        "        def health(self):\n",
        "            r = self.session.get(f\"{self.server_url}/health\", timeout=10)\n",
        "            r.raise_for_status()\n",
        "            return r.json()\n",
        "\n",
        "        def compute_reward(self, model_output, expected_output=None):\n",
        "            r = self.session.post(\n",
        "                f\"{self.server_url}/reward\",\n",
        "                json={\"model_output\": model_output, \"expected_output\": expected_output},\n",
        "                timeout=60,\n",
        "            )\n",
        "            r.raise_for_status()\n",
        "            return r.json()\n",
        "\n",
        "        def reset(self):\n",
        "            r = self.session.post(f\"{self.server_url}/reset\", timeout=30)\n",
        "            r.raise_for_status()\n",
        "            return r.json()\n",
        "\n",
        "    # Test connection\n",
        "    client = RemoteQEMUClient(QEMU_SERVER_URL, QEMU_TOKEN)\n",
        "    try:\n",
        "        health = client.health()\n",
        "        print(f\"✓ Connected to QEMU server\")\n",
        "        print(f\"  VM running: {health.get('vm_running')}\")\n",
        "        print(f\"  Uptime: {health.get('uptime', 0):.1f}s\")\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Connection failed: {e}\")\n",
        "        GRPO_ENABLED = False"
      ],
      "metadata": {
        "id": "setup-grpo-client"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Skip this cell if GRPO is not configured\n",
        "if not GRPO_ENABLED:\n",
        "    print(\"Skipping GRPO training - not configured\")\n",
        "else:\n",
        "    # Load GRPO tasks\n",
        "    grpo_tasks = load_dataset(DATASET_REPO, data_files=\"grpo_tasks.json\")[\"train\"]\n",
        "    print(f\"Loaded {len(grpo_tasks)} GRPO tasks\")\n",
        "\n",
        "    # Create reward function\n",
        "    def reward_function(samples, prompts, outputs, **kwargs):\n",
        "        \"\"\"Compute rewards using remote QEMU execution.\"\"\"\n",
        "        rewards = []\n",
        "        for output in outputs:\n",
        "            try:\n",
        "                result = client.compute_reward(output)\n",
        "                rewards.append(result.get(\"total\", 0.0))\n",
        "                client.reset()  # Reset VM between samples\n",
        "            except Exception as e:\n",
        "                print(f\"Reward error: {e}\")\n",
        "                rewards.append(0.0)\n",
        "        return rewards\n",
        "\n",
        "    print(\"Reward function configured\")"
      ],
      "metadata": {
        "id": "grpo-reward"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Skip this cell if GRPO is not configured\n",
        "if not GRPO_ENABLED:\n",
        "    print(\"Skipping GRPO training - not configured\")\n",
        "else:\n",
        "    from trl import GRPOConfig, GRPOTrainer\n",
        "\n",
        "    # GRPO config for T4\n",
        "    grpo_config = GRPOConfig(\n",
        "        output_dir=\"./plan9-gemma-grpo\",\n",
        "        per_device_train_batch_size=1,\n",
        "        gradient_accumulation_steps=4,\n",
        "        num_train_epochs=1,\n",
        "        learning_rate=1e-5,\n",
        "        logging_steps=1,\n",
        "        num_generations=2,  # Samples per prompt\n",
        "        temperature=0.8,\n",
        "        max_new_tokens=512,\n",
        "        report_to=\"none\",\n",
        "    )\n",
        "\n",
        "    # Format GRPO prompts\n",
        "    def format_grpo_prompt(example):\n",
        "        return {\"prompt\": f\"<start_of_turn>user\\n{example['prompt']}<end_of_turn>\\n<start_of_turn>model\\n\"}\n",
        "\n",
        "    grpo_dataset = grpo_tasks.map(format_grpo_prompt)\n",
        "\n",
        "    # Create GRPO trainer\n",
        "    grpo_trainer = GRPOTrainer(\n",
        "        model=model,\n",
        "        config=grpo_config,\n",
        "        tokenizer=tokenizer,\n",
        "        train_dataset=grpo_dataset,\n",
        "        reward_funcs=[reward_function],\n",
        "    )\n",
        "\n",
        "    print(\"GRPO trainer configured\")\n",
        "    print(\"\\nStarting GRPO training (this will make API calls to your server)...\")\n",
        "\n",
        "    # Train\n",
        "    grpo_trainer.train()\n",
        "\n",
        "    print(\"\\nGRPO training complete!\")"
      ],
      "metadata": {
        "id": "grpo-train"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Resources\n",
        "\n",
        "- [Plan 9 Dataset](https://huggingface.co/datasets/garutyunov/plan9-sft)\n",
        "- [Unsloth Documentation](https://github.com/unslothai/unsloth)\n",
        "- [TRL Documentation](https://huggingface.co/docs/trl)\n",
        "- [9ml Project](https://github.com/9ml/9ml)"
      ],
      "metadata": {
        "id": "resources"
      }
    }
  ]
}
